{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e42db2bf",
   "metadata": {},
   "source": [
    "# 12. Practical lesson: Automatically extract information from a batch of files\n",
    "\n",
    "In the previous lessons, we have seen how we can extract information from the Alto, Didl, Tei and Page XML files. \n",
    "In this lesson, we will show you how you can use the codes from the previous lessons and, with a little bit of alteration, use them to automatically extract content from batches of XML files and save them as either textfiles or csv files. \n",
    "\n",
    "We will provide the following examples:\n",
    "\n",
    "- Extract complete page content with newspaper metadata from various Alto and corresponding Didle files <span style=\"color:#ef6079\">(*basic*)</span>.\n",
    "- Extract the poems from various Tei files and store them in seperate csv files per book <span style=\"color:#ef6079\">(*moderate*)</span>.\n",
    "- Extract the content, including reading order, from various Page files and store the content in csv files <span style=\"color:#ef6079\">(*advanced*)</span>.\n",
    "\n",
    "##  Extract complete page content with newspaper metadata from various Alto and corresponding Didle files.\n",
    "\n",
    "In lesson 7, we used the following code to extract the content and page number from a newspaper Alto XML and the title and publication year from the corresponding Didle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f698b43",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree_alto = ET.parse('data/alto_id1.xml')\n",
    "root_alto = tree_alto.getroot()\n",
    "\n",
    "tree_didl = ET.parse('data/didl_id1.xml')\n",
    "root_didl = tree_didl.getroot()\n",
    "\n",
    "ns_alto = {'ns0': 'http://schema.ccs-gmbh.com/ALTO'} \n",
    "\n",
    "ns_didl = {'dc': 'http://purl.org/dc/elements/1.1/',\n",
    "          'ns2': 'urn:mpeg:mpeg21:2002:02-DIDL-NS', \n",
    "          'ns4' : 'info:srw/schema/1/dc-v1.1' }\n",
    "\n",
    "article_content = \"\"\n",
    "\n",
    "for book in root_alto.findall('.//ns0:TextBlock', ns_alto):\n",
    "    for article in book.findall('.//ns0:String', ns_alto):\n",
    "        content = article.get('CONTENT')\n",
    "        article_content = article_content + content\n",
    "    article_content = article_content + \"\\n\"\n",
    "\n",
    "    \n",
    "for book in root_alto.findall('.//ns0:Page', ns_alto):\n",
    "    pagenr = book.get('ID')\n",
    "    \n",
    "item = root_didl.find('.//ns2:Resource', ns_didl)\n",
    "    \n",
    "for article in item.findall('.//ns4:dcx', ns_didl):\n",
    "    title = article.find('.//dc:title', ns_didl).text\n",
    "    date = article.find('.//dc:date', ns_didl).text\n",
    "\n",
    "        \n",
    "filename = f'{title}_{date}_{pagenr}.txt'\n",
    "\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(article_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2e1a5",
   "metadata": {},
   "source": [
    "With few little alterations, we can use this code to automatically work with a batch of files. \n",
    "\n",
    "- We need a code that automatically search through folder in your computer;\n",
    "- We need to add a piece of code that finds the corresponding Didle file for every alto file. \n",
    "\n",
    "For the following code, we assume you have a folder called 'alto', which contains the Alto XML files, and a folder\n",
    "'didl' that contains the Didl files ([downloaded here](https://github.com/MirjamC/xml-workshop/tree/master/data)). Both alto and didle have a filename that starts with an identifier, followed by \n",
    "either _alto or _didl. Make sure that there are no other files in the folder.\n",
    "\n",
    "We start with a little loop that runs through your alto folder an returns all the file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307fcc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ddd_010097934_alto.xml\n",
      "ddd_010097935_alto.xml\n",
      "ddd_010097936_alto.xml\n",
      "ddd_010097937_alto.xml\n",
      "ddd_010097938_alto.xml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# assign directory\n",
    "directory = 'data/alto/'\n",
    " \n",
    "for filename in os.listdir(directory):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc16eb",
   "metadata": {},
   "source": [
    "Now we need a code to strip the identifier from the alto file, and create the filename for the Didl file. \n",
    "This can be done with string alterations in Python, as we also did in lesson 7 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a2847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ddd_010097934_alto.xml_didl.xml\n"
     ]
    }
   ],
   "source": [
    "filename = 'ddd_010097934_alto.xml'\n",
    "filename_didl = filename.split('_alto')[0] ## Split the string by the underscore and only keep the first part\n",
    "filename_didl = filename + \"_didl.xml\"\n",
    "print(filename_didl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096787ae",
   "metadata": {},
   "source": [
    "Now, we have a way to retreive all alto files and corresponding Didl files, so the only thing left is to put it in one big loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e8b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "directory_alto = 'data/alto/'\n",
    "directory_didl = 'data/didl/'\n",
    " \n",
    "for filename in os.listdir(directory):\n",
    "\n",
    "    tree_alto = ET.parse(directory_alto + filename)\n",
    "    root_alto = tree_alto.getroot()\n",
    "    \n",
    "    filename_didl = filename.split('_alto')[0] ## Split the string by the underscore and only keep the first part\n",
    "    filename_didl = filename_didl + \"_didl.xml\"\n",
    "\n",
    "    tree_didl = ET.parse(directory_didl + filename_didl)\n",
    "    root_didl = tree_didl.getroot()\n",
    "\n",
    "    ns_alto = {'ns0': 'http://schema.ccs-gmbh.com/ALTO'} \n",
    "\n",
    "    ns_didl = {'dc': 'http://purl.org/dc/elements/1.1/',\n",
    "              'ns2': 'urn:mpeg:mpeg21:2002:02-DIDL-NS', \n",
    "              'ns4' : 'info:srw/schema/1/dc-v1.1' }\n",
    "\n",
    "    article_content = \"\"\n",
    "\n",
    "    for book in root_alto.findall('.//ns0:TextBlock', ns_alto):\n",
    "        for article in book.findall('.//ns0:String', ns_alto):\n",
    "            content = article.get('CONTENT')\n",
    "            article_content = article_content + content\n",
    "        article_content = article_content + \"\\n\"\n",
    "\n",
    "\n",
    "    for book in root_alto.findall('.//ns0:Page', ns_alto):\n",
    "        pagenr = book.get('ID')\n",
    "\n",
    "    item = root_didl.find('.//ns2:Resource', ns_didl)\n",
    "\n",
    "    for article in item.findall('.//ns4:dcx', ns_didl):\n",
    "        title = article.find('.//dc:title', ns_didl).text\n",
    "        date = article.find('.//dc:date', ns_didl).text\n",
    "\n",
    "\n",
    "    filename = f'{title}_{date}_{pagenr}.txt'\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(article_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee2ace",
   "metadata": {},
   "source": [
    "## Extract the poems from various Tei files and store them in seperate csv files per book.\n",
    "\n",
    "In lesson 10, we extracted poems from a Tei file and stored them in a csv file. The code for this looked like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf3be16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"data/tei.xml\", encoding='utf8') as f:\n",
    "    root = BeautifulSoup(f, 'xml')\n",
    "\n",
    "poem_list = []\n",
    "counter = 1\n",
    "\n",
    "for div in root.find_all('lg'):\n",
    "    if div.get('type') == 'poem':\n",
    "        poem = \"poem_\" + str(counter)\n",
    "        content = div.text\n",
    "        poem_list.append([poem, content])\n",
    "        counter += 1\n",
    "        \n",
    "poems = pd.DataFrame(poem_list , columns = (['poem', 'content']))\n",
    "\n",
    "poems.to_csv('poems.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ffb50",
   "metadata": {},
   "source": [
    "Just as we did with the alto files in the previous section, we can do the same for the TEI files. \n",
    "\n",
    "For the following exercises, we assume you have a folder on your computer with the name 'tei', in which you stored the various tei files ([downloaded here](https://github.com/MirjamC/xml-workshop/tree/master/data)). \n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: attention\n",
    "what steps do we need to take to be able to create a batch output?\n",
    "```\n",
    "\n",
    "````{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "- create a loop that runs through the tei folder \n",
    "- create a file name for each file, based on the identifier\n",
    "````\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: attention\n",
    "Create a loop that runs through the files in your tei folder and print their names.\n",
    "```\n",
    "\n",
    "````{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "```Python\n",
    "import os\n",
    "# assign directory\n",
    "directory = 'data/tei/'\n",
    " \n",
    "for filename in os.listdir(directory):\n",
    "    print(filename)\n",
    "```\n",
    "````\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: attention\n",
    "create the variable 'filename' with the value 'bild001dich01_01.xml'. Strip the filename from the suffix .xml \n",
    "and print the filename.    \n",
    "```\n",
    "\n",
    "````{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "```Python\n",
    "filename = 'bild001dich01_01.xml'\n",
    "filename = filename.split('.')[0]\n",
    "print(filename)\n",
    "```\n",
    "````\n",
    "\n",
    "Now we have all ingredients to automatically extracts the poems from the batch of tei files, and save them as csv\n",
    "with their identifier as name. \n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: attention\n",
    "Write a code that loops through the tei files, extracts the poems and stores them as csv.     \n",
    "```\n",
    "\n",
    "````{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "```Python\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# assign directory\n",
    "directory = 'data/tei/'\n",
    " \n",
    "for filename in os.listdir(directory):\n",
    "    with open(directory + filename, encoding='utf8') as f:\n",
    "        root = BeautifulSoup(f, 'xml')\n",
    "\n",
    "    identifier = filename.split('.')[0]\n",
    "    \n",
    "    poem_list = []\n",
    "    counter = 1\n",
    "\n",
    "    for div in root.find_all('lg'):\n",
    "        if div.get('type') == 'poem':\n",
    "            poem = \"poem_\" + str(counter)\n",
    "            content = div.text\n",
    "            poem_list.append([poem, content])\n",
    "            counter += 1\n",
    "\n",
    "    poems = pd.DataFrame(poem_list , columns = ['poem', 'content'])\n",
    "\n",
    "    poems.to_csv(identifier + '.csv')\n",
    "```\n",
    "````\n",
    "\n",
    "## Extract the content, including reading order, from various Page files and store the content in csv files\n",
    "\n",
    "And off course, we can do the same for the page XML. Lets start by repeating the code we made to extract the content, including the region information, and save it to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dffc98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "tree = ET.parse('data/page.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "ns = {'ns0': 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2010-03-19'}\n",
    "\n",
    "dict_order = {}\n",
    "\n",
    "for order in root.findall('.//ns0:ReadingOrder', ns):\n",
    "    for group in root.findall('.//ns0:OrderedGroup', ns):\n",
    "        groupnr = group.get('id')\n",
    "        for suborder in group.findall('.//ns0:RegionRefIndexed', ns):  \n",
    "            region = suborder.get('regionRef')\n",
    "            index = suborder.get('index')\n",
    "            dict_order.setdefault(region,[]).append([groupnr, index])\n",
    "                \n",
    "content_list = []\n",
    "\n",
    "for newspaper in root.findall('.//ns0:TextRegion', ns):\n",
    "    region = newspaper.get('id')\n",
    "    if region in dict_order:\n",
    "        groupvalues = dict_order[region]\n",
    "        group = groupvalues[0][0]\n",
    "        index = groupvalues[0][1]\n",
    "    else:\n",
    "        group = 0\n",
    "        index = 0\n",
    "    for content in newspaper.findall('.//ns0:Unicode', ns):\n",
    "        content = content.text\n",
    "    content_list.append([group, index, region, content])\n",
    "\n",
    "newspaper_with_order = pd.DataFrame(content_list, columns = [\"Group\", \"Index\", \"Region\", \"Content\"]) \n",
    "newspaper_with_order = newspaper_with_order.sort_values(['Group', 'Index'], ascending = [True, True])\n",
    "\n",
    "newspaper_with_order.to_csv('newspaper_with_order.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ce4b07",
   "metadata": {},
   "source": [
    "For the following exercises, we assume you have a folder on your computer with the name 'page', in which you stored the various page files ([downloaded here](https://github.com/MirjamC/xml-workshop/tree/master/data)).\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: attention\n",
    "Write a code that loops through the page files in your 'page' folder, extracts the content with region information, and\n",
    "store them in a .csv file with as name the identifier of the page file.     \n",
    "```\n",
    "\n",
    "````{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "```Python\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "directory = 'data/page/'\n",
    " \n",
    "for filename in os.listdir(directory):\n",
    "\n",
    "    tree = ET.parse(directory + filename)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    identifier = filename.split('.')[0]\n",
    "    print(filename)\n",
    "\n",
    "    ns = {'ns0': 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2010-03-19'}\n",
    "\n",
    "    dict_order = {}\n",
    "\n",
    "    for order in root.findall('.//ns0:ReadingOrder', ns):\n",
    "        for group in root.findall('.//ns0:OrderedGroup', ns):\n",
    "            groupnr = group.get('id')\n",
    "            for suborder in group.findall('.//ns0:RegionRefIndexed', ns):  \n",
    "                region = suborder.get('regionRef')\n",
    "                index = suborder.get('index')\n",
    "                dict_order.setdefault(region,[]).append([groupnr, index])\n",
    "                \n",
    "    content_list = []\n",
    "\n",
    "    for newspaper in root.findall('.//ns0:TextRegion', ns):\n",
    "        region = newspaper.get('id')\n",
    "        if region in dict_order:\n",
    "            groupvalues = dict_order[region]\n",
    "            group = groupvalues[0][0]\n",
    "            index = groupvalues[0][1]\n",
    "        else:\n",
    "            group = 0\n",
    "            index = 0\n",
    "        for content in newspaper.findall('.//ns0:Unicode', ns):\n",
    "            content = content.text\n",
    "        content_list.append([group, index, region, content])\n",
    "\n",
    "    newspaper_with_order = pd.DataFrame(content_list, columns = [\"Group\", \"Index\", \"Region\", \"Content\"]) \n",
    "    newspaper_with_order = newspaper_with_order.sort_values(['Group', 'Index'], ascending = [True, True])\n",
    "\n",
    "\n",
    "    newspaper_with_order.to_csv(identifier + '.csv')\n",
    "\n",
    "```\n",
    "````\n",
    "\n",
    "And that's all! \n",
    "You have now seen multiple ways of automatically extracting content from batches of files which can save a lot of time and errors."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "source_map": [
   14,
   31,
   70,
   83,
   90,
   95,
   100,
   104,
   151,
   157,
   177,
   270,
   308
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}